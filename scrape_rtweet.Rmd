---
title: "Scrape with rtweet"
author: "Zhaohan Dong"
date: '2022-05-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

We will use the librarires rtweet and tidyverse.

```{r message=FALSE}
library(rtweet, quietly = TRUE)
library(tidyverse, quietly = TRUE)
```

## Load credentials

Load credentials for twitter API.

```{r pressure, echo=FALSE}
authentication <- readRDS("data/twitter_api_keys.rds")
twitter_token <- create_token(app = "ESG-Academic",
                              consumer_key = authentication$consumer_key,
                              consumer_secret = authentication$consumer_secret)
```

## Load target tweet IDs

Load the tweet_id from the csv file and store as strings.

```{r}
tweet_id <- read_csv("data/1_Tweet_IDs.csv", col_types = "c")
```

## Define twitter look up function

```{r}
batch_lookup <- function(tweet_list) {
  # The lookup_statuses function only takes vectors, not strings
  lookup_statuses(unlist(tweet_list))
}
```

## Loop through the tweet IDs

```{r}
i <- 1
if (!exists("tweet_collection")) {
  tweet_collection <- tibble()
}

while (i < 14353859) {
  # Start process timing
  ptm <- proc.time()
  
  # Start querying data
  print(paste("Processing", i, "to", i + 89999, "tweet ID"))
  result <- batch_lookup(tweet_id[i:(i + 89999),])
<<<<<<< HEAD
  print(paste("Query time", proc.time() - ptm))
  
  # Saving data to csv
=======
  print(proc.time() - ptm)
>>>>>>> a70dc14c481f97920645c35b1c8e1d308efa2bb3
  filename <- paste("tweet_", i, ".csv", sep = "")
  save_as_csv(result, filename)
  print("CSV saved.")
  i <- i+90000
  print(paste(i, "tweet ID processed"))
  # Stop process timing
<<<<<<< HEAD
  print(paste("Total batch time", proc.time() - ptm))
=======
  print(proc.time() - ptm)
>>>>>>> a70dc14c481f97920645c35b1c8e1d308efa2bb3
  Sys.sleep(500)
}
```
